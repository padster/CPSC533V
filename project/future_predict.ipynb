{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future predict\n",
    "\n",
    "Given a replay of expert trajectory (s, a)... try to predict s' from s\n",
    "\n",
    "May help to provide a better BC network, or roll out imagined trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCLUDE_ACTION_INPUT = False\n",
    "\n",
    "NORMALIZE_STATE = True\n",
    "NORMALIZE_ACTIONS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "sys.path.append('D:/projects/carball')\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import carball\n",
    "from carball.controls.controls import ControlsCreator\n",
    "from carball.json_parser import Game\n",
    "\n",
    "# NN \n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading...\n",
      "\treplays\\noBoost1v1_1.replay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find field_of_view in camera settings for Sundown\n",
      "Could not find height in camera settings for Sundown\n",
      "Could not find pitch in camera settings for Sundown\n",
      "Could not find distance in camera settings for Sundown\n",
      "Could not find stiffness in camera settings for Sundown\n",
      "Could not find swivel_speed in camera settings for Sundown\n",
      "Could not find transition_speed in camera settings for Sundown\n",
      "D:/projects/carball\\carball\\controls\\rotations.py:87: RuntimeWarning: invalid value encountered in sign\n",
      "  rhs[1] / (T_p + np.sign(rhs[1]) * omega[1] * D_p),\n",
      "D:/projects/carball\\carball\\controls\\rotations.py:88: RuntimeWarning: invalid value encountered in sign\n",
      "  rhs[2] / (T_y - np.sign(rhs[2]) * omega[2] * D_y)\n"
     ]
    }
   ],
   "source": [
    "def replayToGame(replayID):\n",
    "    replayPath = os.path.join(\"replays\", \"%s.replay\" % replayID)\n",
    "    print (\"Loading...\\n\\t%s\" % replayPath)\n",
    "    json = carball.decompile_replay(replayPath)\n",
    "    game = Game()\n",
    "    game.initialize(loaded_json=json)\n",
    "    return game\n",
    "\n",
    "GAME = replayToGame(\"noBoost1v1_1\")\n",
    "# Parses action-related properties into their actual actions\n",
    "ControlsCreator().get_controls(GAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 players loaded!\n",
      "\n",
      "Orange team:\n",
      "\tSundown\n",
      "\n",
      "Blue team:\n",
      "\tpadster\n",
      "\n",
      "12921 data points acquired\n"
     ]
    }
   ],
   "source": [
    "nPlayers = len(GAME.players)\n",
    "#assert nPlayers == 6, \"Only 3v3 modes supported, this has %d players\" % nPlayers\n",
    "assert nPlayers == 2, \"Only 1v1 modes supported, this has %d players\" % nPlayers\n",
    "print (\"%d players loaded!\" % nPlayers)\n",
    "\n",
    "orangeIdx = [i for i, p in enumerate(GAME.players) if p.is_orange]\n",
    "blueIdx = [i for i, p in enumerate(GAME.players) if not p.is_orange]\n",
    "\n",
    "print (\"\\nOrange team:\")\n",
    "for i in orangeIdx:\n",
    "    print (\"\\t%s\" % GAME.players[i].name)\n",
    "print (\"\\nBlue team:\")\n",
    "for i in blueIdx:\n",
    "    print (\"\\t%s\" % GAME.players[i].name)\n",
    "        \n",
    "nTimepoints = len(GAME.ball)\n",
    "for p in GAME.players:\n",
    "    assert len(p.data) >= nTimepoints - 20, \\\n",
    "        \"Players (%d) need the same number of time points (%d), no leaves allowed\" % (len(p.data), nTimepoints)\n",
    "    nTimepoints = min(nTimepoints, len(p.data))\n",
    "print (\"\\n%d data points acquired\" % nTimepoints)\n",
    "\n",
    "# if not all the same, trim\n",
    "GAME.ball = GAME.ball.tail(nTimepoints)\n",
    "for p in GAME.players:\n",
    "    p.data = p.data.tail(nTimepoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ed23f1040949d2b92801215a156f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "PLAYER_STATE_KEYS = (\n",
    "    ['pos_x', 'pos_y', 'pos_z'] +              # x \\\n",
    "    ['vel_x', 'vel_y', 'vel_z'] +              # dx/xt \\\n",
    "    ['rot_x', 'rot_y', 'rot_z'] +              # q \\\n",
    "    ['ang_vel_x', 'ang_vel_y', 'ang_vel_z'] +  # dq/dt \\\n",
    "    ['boost']\n",
    ")\n",
    "\n",
    "PLAYER_ANALOG_ACTION_KEYS = ['throttle', 'steer']\n",
    "PLAYER_DIGITAL_ACTION_KEYS = ['boost'] # Simplified easier case\n",
    "#PLAYER_DIGITAL_ACTION_KEYS = ['boost', 'jump', 'handbreak'] # Harder case\n",
    "PLAYER_ACTION_KEYS = PLAYER_ANALOG_ACTION_KEYS + PLAYER_DIGITAL_ACTION_KEYS\n",
    "\n",
    "# Only useful state for ball is (x,y,z) position\n",
    "BALL_STATE_KEYS = (\n",
    "    ['pos_x', 'pos_y', 'pos_z']\n",
    ")\n",
    "\n",
    "def normMiddlePeak(v):\n",
    "    return 4/(1 + np.exp(-v)) - 2\n",
    "    #return np.cbrt(v)\n",
    "    #return math.erf(v)\n",
    "    \n",
    "def ensure01(values):\n",
    "    assert np.sum(values.isin([0, 1])) == len(values), \"Boolean column is not just 0s and 1s\"\n",
    "    return values\n",
    "    \n",
    "\n",
    "def imputePlayerState(stateDF):\n",
    "    # Note: Missing values come from start of game or after explosion, in which case, \n",
    "    # should be an okay approximation to impute by forward then backfilling.\n",
    "    return stateDF.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "def normPlayerState(stateDF):\n",
    "    normedDF = pd.DataFrame()\n",
    "    normedDF['pos_x'] = stateDF['pos_x'] / 4000.0\n",
    "    normedDF['pos_y'] = stateDF['pos_y'] / 5500.0\n",
    "    normedDF['pos_z'] = stateDF['pos_z'] / 2000.0\n",
    "    normedDF['vel_x'] = normMiddlePeak(stateDF['vel_x'] / 22000.0)\n",
    "    normedDF['vel_y'] = normMiddlePeak(stateDF['vel_y'] / 22000.0)\n",
    "    normedDF['vel_z'] = normMiddlePeak(normMiddlePeak(stateDF['vel_z'] / 16000.0))\n",
    "    normedDF['rot_x'] = normMiddlePeak(stateDF['rot_x'] / (np.pi / 2))\n",
    "    normedDF['rot_y'] = stateDF['rot_y'] / np.pi\n",
    "    normedDF['rot_z'] = normMiddlePeak(stateDF['rot_z'] / np.pi)\n",
    "    normedDF['ang_vel_x'] = stateDF['ang_vel_x'] / 6000.0\n",
    "    normedDF['ang_vel_y'] = stateDF['ang_vel_y'] / 6000.0\n",
    "    normedDF['ang_vel_z'] = stateDF['ang_vel_z'] / 6000.0\n",
    "    normedDF['boost'] = stateDF['boost'] / 256.0\n",
    "    assert normedDF.shape[1] == stateDF.shape[1], \"Columns are missing normalization\"\n",
    "    return normedDF\n",
    "\n",
    "def imputePlayerActions(actionDF):\n",
    "    # Missing value here are converted to inaction\n",
    "    actionDF = actionDF.fillna({\n",
    "        'throttle': 0,\n",
    "        'steer': 0,\n",
    "        'jump': False,\n",
    "        'boost': False,\n",
    "        'handbrake': False,\n",
    "    })\n",
    "    #actionDF = actionDF.astype({\n",
    "    #    'jump': 'float64',               # from bool\n",
    "    #    'boost': 'float64',              # from bool\n",
    "    #    'handbrake': 'float64',          # from bool\n",
    "    #})\n",
    "    return actionDF\n",
    "\n",
    "def normPlayerActions(actionDF):\n",
    "    # Note: ControlsCreator already norms them for us\n",
    "    normedDF = actionDF.copy()\n",
    "    assert normedDF.shape[1] == actionDF.shape[1], \"Columns are missing normalization\"\n",
    "    return normedDF\n",
    "\n",
    "def normBallState(stateDF):\n",
    "    normedDF = pd.DataFrame()\n",
    "    normedDF['pos_x'] = stateDF['pos_x'] / 4000.0\n",
    "    normedDF['pos_y'] = stateDF['pos_y'] / 5500.0\n",
    "    normedDF['pos_z'] = stateDF['pos_z'] / 2000.0\n",
    "    assert normedDF.shape[1] == stateDF.shape[1], \"Columns are missing normalization\"\n",
    "    return normedDF\n",
    "\n",
    "def cleanPlayerStates(playerDF):\n",
    "    stateDF = playerDF[PLAYER_STATE_KEYS]\n",
    "    stateDF = imputePlayerState(stateDF)\n",
    "    if NORMALIZE_STATE:\n",
    "        stateDF = normPlayerState(stateDF)\n",
    "    return stateDF\n",
    "\n",
    "def cleanPlayerActions(controlDF):\n",
    "    actionDF = controlDF[PLAYER_ACTION_KEYS]\n",
    "    actionDF = imputePlayerActions(actionDF)\n",
    "    if NORMALIZE_ACTIONS:\n",
    "        actionDF = normPlayerActions(actionDF)\n",
    "    return actionDF\n",
    "\n",
    "def cleanBallStates(ballDF):\n",
    "    stateDF = ballDF[BALL_STATE_KEYS] * 1.0 # force int -> float\n",
    "    if NORMALIZE_STATE:\n",
    "        stateDF = normBallState(stateDF)\n",
    "    return stateDF\n",
    "    \n",
    "playerStates, playerActions = [], []\n",
    "for p in tqdm(GAME.players):\n",
    "    playerStates.append( cleanPlayerStates( p.data))\n",
    "    playerActions.append(cleanPlayerActions(p.controls))\n",
    "ballStates = cleanBallStates(GAME.ball)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12921, 29) float64\n",
      "(12921, 3) object\n"
     ]
    }
   ],
   "source": [
    "def copyIntoPrefixed(toDF, fromDF, prefix):\n",
    "    for column in list(fromDF):\n",
    "        toDF[prefix + column] = fromDF[column]\n",
    "\n",
    "def teamBreakdown(playerIdx):\n",
    "    isOrange = GAME.players[playerIdx].is_orange\n",
    "    teamIdx = [i for i in range(nPlayers) if i != playerIdx and GAME.players[i].is_orange == isOrange]\n",
    "    enemyIdx = blueIdx if isOrange else orangeIdx\n",
    "    return teamIdx, enemyIdx \n",
    "    \n",
    "def stateAndActionsForPlayer(playerIdx):\n",
    "    stateDF = pd.DataFrame(index=ballStates.index)\n",
    "    copyIntoPrefixed(stateDF, ballStates, \"b_\")\n",
    "    copyIntoPrefixed(stateDF, playerStates[playerIdx], \"me_\")\n",
    "    \n",
    "    teamIdx, enemyIdx = teamBreakdown(playerIdx)\n",
    "    for i, idx in enumerate(teamIdx):\n",
    "        copyIntoPrefixed(stateDF, playerStates[idx], \"t%d_\" % i)\n",
    "    for i, idx in enumerate(enemyIdx):\n",
    "        copyIntoPrefixed(stateDF, playerStates[idx], \"e%d_\" % i)\n",
    "    assert max(stateDF.isna().sum()) == 0, \"NA state values not successfully removed?\"\n",
    "    \n",
    "    actionDF = playerActions[playerIdx]\n",
    "    assert max(actionDF.isna().sum()) == 0, \"NA action values not successfully removed?\"\n",
    "    return stateDF, actionDF\n",
    "    \n",
    "P2S, P2A = stateAndActionsForPlayer(1)\n",
    "print (P2S.values.shape, P2S.values.dtype)\n",
    "print (P2A.values.shape, P2A.values.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        # Start with a super simple multi-layer perceptron, one hidden layer \n",
    "        self.dimH       = 32 # hidden layer has 16 dimensions\n",
    "        self.dimIn      = state_size + (action_size if INCLUDE_ACTION_INPUT else 0)\n",
    "        self.dimOut     = state_size\n",
    "\n",
    "        self.model = torch.nn.Sequential(\n",
    "           nn.Linear(self.dimIn, self.dimH),\n",
    "           nn.ReLU(),\n",
    "           nn.Linear(self.dimH, self.dimOut),\n",
    "        )\n",
    "        self.model.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input data type needs to be converted to float\n",
    "        return self.model(x.float())\n",
    "        \n",
    "    def save(self, modelID):\n",
    "        path = os.path.join(\"models\", \"%s.pt\" % modelID)\n",
    "        torch.save(self.state_dict(), path)\n",
    "        print('Saved model!\\n\\t%s' % path)\n",
    "        \n",
    "    def load(self, modelID):\n",
    "        path = os.path.join(\"models\", \"%s.pt\" % modelID)\n",
    "        self.load_state_dict(torch.load(path))\n",
    "        print('Loaded model!\\n\\t%s' % path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch   10/500] [iter     650] [loss 0.06214]\n",
      "[epoch   20/500] [iter    1300] [loss 0.02968]\n",
      "[epoch   30/500] [iter    1950] [loss 0.01596]\n",
      "[epoch   40/500] [iter    2600] [loss 0.01114]\n",
      "[epoch   50/500] [iter    3250] [loss 0.00865]\n",
      "[epoch   60/500] [iter    3900] [loss 0.00559]\n",
      "[epoch   70/500] [iter    4550] [loss 0.00562]\n",
      "[epoch   80/500] [iter    5200] [loss 0.00402]\n",
      "[epoch   90/500] [iter    5850] [loss 0.00583]\n",
      "[epoch  100/500] [iter    6500] [loss 0.00499]\n",
      "[epoch  110/500] [iter    7150] [loss 0.00417]\n",
      "[epoch  120/500] [iter    7800] [loss 0.00414]\n",
      "[epoch  130/500] [iter    8450] [loss 0.00281]\n",
      "[epoch  140/500] [iter    9100] [loss 0.00521]\n",
      "[epoch  150/500] [iter    9750] [loss 0.00353]\n",
      "[epoch  160/500] [iter   10400] [loss 0.00281]\n",
      "[epoch  170/500] [iter   11050] [loss 0.00284]\n",
      "[epoch  180/500] [iter   11700] [loss 0.00523]\n",
      "[epoch  190/500] [iter   12350] [loss 0.00376]\n",
      "[epoch  200/500] [iter   13000] [loss 0.00521]\n",
      "[epoch  210/500] [iter   13650] [loss 0.00279]\n",
      "[epoch  220/500] [iter   14300] [loss 0.00295]\n",
      "[epoch  230/500] [iter   14950] [loss 0.00182]\n",
      "[epoch  240/500] [iter   15600] [loss 0.00343]\n",
      "[epoch  250/500] [iter   16250] [loss 0.00348]\n",
      "[epoch  260/500] [iter   16900] [loss 0.00243]\n",
      "[epoch  270/500] [iter   17550] [loss 0.00208]\n",
      "[epoch  280/500] [iter   18200] [loss 0.00215]\n",
      "[epoch  290/500] [iter   18850] [loss 0.00443]\n",
      "[epoch  300/500] [iter   19500] [loss 0.00398]\n",
      "[epoch  310/500] [iter   20150] [loss 0.00363]\n",
      "[epoch  320/500] [iter   20800] [loss 0.00363]\n",
      "[epoch  330/500] [iter   21450] [loss 0.00276]\n",
      "[epoch  340/500] [iter   22100] [loss 0.00390]\n",
      "[epoch  350/500] [iter   22750] [loss 0.00424]\n",
      "[epoch  360/500] [iter   23400] [loss 0.00496]\n",
      "[epoch  370/500] [iter   24050] [loss 0.00400]\n",
      "[epoch  380/500] [iter   24700] [loss 0.00265]\n",
      "[epoch  390/500] [iter   25350] [loss 0.00219]\n",
      "[epoch  400/500] [iter   26000] [loss 0.00230]\n",
      "[epoch  410/500] [iter   26650] [loss 0.00235]\n",
      "[epoch  420/500] [iter   27300] [loss 0.00357]\n",
      "[epoch  430/500] [iter   27950] [loss 0.00493]\n",
      "[epoch  440/500] [iter   28600] [loss 0.00243]\n",
      "[epoch  450/500] [iter   29250] [loss 0.00610]\n",
      "[epoch  460/500] [iter   29900] [loss 0.00251]\n",
      "[epoch  470/500] [iter   30550] [loss 0.00208]\n",
      "[epoch  480/500] [iter   31200] [loss 0.00309]\n",
      "[epoch  490/500] [iter   31850] [loss 0.00195]\n",
      "[epoch  500/500] [iter   32500] [loss 0.00250]\n",
      "Saved model!\n",
      "\tmodels\\fp_BoostOnly1v1.pt\n"
     ]
    }
   ],
   "source": [
    "TOTAL_EPOCHS = 500\n",
    "BATCH_SZ = 200\n",
    "#PRINT_INTERVAL = 1000\n",
    "LOG_INTERVAL = 1000\n",
    "LEARNING_RATE = 0.0001\n",
    "REGULARIZER_WEIGHT = 3e-4\n",
    "\n",
    "# Epochs are fast, only print every 10\n",
    "EPOCH_PRINTERVAL = 10\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "def train_future_predict(dataBatches, model):\n",
    "    # Adam optimizer usually a good default.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=REGULARIZER_WEIGHT)\n",
    "    \n",
    "    # MSE loss for [-1, 1] continuous variables:\n",
    "    analog_loss_func = torch.nn.MSELoss().to(device)\n",
    "    \n",
    "    # Cross entropy loss for binary variables:\n",
    "    #digital_loss_func = torch.nn.BCEWithLogitsLoss().to(device)# weight=torch.tensor(1.0), pos_weight=torch.tensor(1.0)).to(device)\n",
    "\n",
    "    gradient_steps = 0\n",
    "\n",
    "    for epoch in range(1, TOTAL_EPOCHS + 1):\n",
    "        batchShuffled = random.sample(dataBatches, len(dataBatches))\n",
    "        lastLoss = -1\n",
    "        for iteration, data in enumerate(batchShuffled):\n",
    "            data = {k: v.to(device) for k, v in data.items()}\n",
    "            #print (data)\n",
    "\n",
    "            s, sPrime = data['input'], data['output']\n",
    "            predicted = model(data['input'])\n",
    "            \n",
    "            loss = analog_loss_func(sPrime, predicted)\n",
    "            \n",
    "\n",
    "            #loss = loss_function(output, data[\"action\"])\n",
    "            #aLoss = W_ALOSS * analog_loss_func(output[:aasz], analogAction)\n",
    "            #dLoss = W_DLOSS * digital_loss_func(output[aasz:], digitalAction)\n",
    "            #loss = aLoss + dLoss\n",
    "            #print (aLoss, dLoss, loss)\n",
    "            #loss = dLoss\n",
    "            #print (output[aasz:], digitalAction)\n",
    "            #print (loss.item())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #if gradient_steps % PRINT_INTERVAL == 0:\n",
    "            #    print('[epoch {:4d}/{}] [iter {:7d}] [loss {:.5f}]'\n",
    "            #        .format(epoch, TOTAL_EPOCHS, gradient_steps, loss.item()))\n",
    "            if gradient_steps % LOG_INTERVAL == 0:\n",
    "                #writer.add_scalar('analogLoss', aLoss.item(), gradient_steps)\n",
    "                #writer.add_scalar('digitalLoss', dLoss.item(), gradient_steps)\n",
    "                writer.add_scalar('loss', loss.item(), gradient_steps)\n",
    "            \n",
    "            gradient_steps += 1\n",
    "            \n",
    "            if iteration == len(batchShuffled) - 1:\n",
    "                lastLoss = loss.item()\n",
    "                \n",
    "        if epoch % EPOCH_PRINTERVAL == 0:\n",
    "            print ('[epoch {:4d}/{}] [iter {:7d}] [loss {:.5f}]'.format(\n",
    "               epoch, TOTAL_EPOCHS, gradient_steps, lastLoss)\n",
    "            )\n",
    "\n",
    "        #if epoch % TEST_INTERVAL == 0:\n",
    "        #    score = eval_policy(policy=model, env=ENV_NAME)\n",
    "        #    print('[Test on environment] [epoch {}/{}] [score {:.2f}]'\n",
    "        #        .format(epoch, TOTAL_EPOCHS, score))\n",
    "\n",
    "\n",
    "    # Force directory to be same as this file.\n",
    "    #model_name = \"behavioral_cloning_{}.pt\".format(ENV_NAME)\n",
    "    #fullPath = os.path.join(os.path.dirname(__file__), model_name)\n",
    "    #torch.save(model.state_dict(), fullPath)\n",
    "    #print('Saving model to {}'.format(fullPath))\n",
    "    \n",
    "    \n",
    "random.seed(1234)\n",
    "def dataToBatches(states, actions, batchSz):\n",
    "    nRows = states.shape[0]\n",
    "    stateSz = states.shape[1]\n",
    "    actionSz = len(PLAYER_ACTION_KEYS)\n",
    "    \n",
    "    inputSz = stateSz + (actionSz if INCLUDE_ACTION_INPUT else 0)\n",
    "    outputSz = stateSz\n",
    "\n",
    "    sPrimeOrder = list(range(1, nRows))\n",
    "    random.shuffle(sPrimeOrder)\n",
    "    \n",
    "    dataBatches = []\n",
    "    for i in range(0, len(sPrimeOrder), batchSz):\n",
    "        nInBatch = min(batchSz, len(sPrimeOrder) - i)\n",
    "        batchIn = np.zeros((nInBatch, inputSz))\n",
    "        batchOut = np.zeros((nInBatch, outputSz))\n",
    "        \n",
    "        for j in range(nInBatch):\n",
    "            sPrimeAt = sPrimeOrder[i + j]\n",
    "            sPrime = states.iloc[sPrimeAt, :].values\n",
    "            s = states.iloc[sPrimeAt - 1, :].values\n",
    "            a = actions.iloc[sPrimeAt - 1, :].values\n",
    "            \n",
    "            output_j = sPrime\n",
    "            input_j = s\n",
    "            if INCLUDE_ACTION_INPUT:\n",
    "                input_j = np.hstack(inp)\n",
    "            batchIn[j, :] = input_j\n",
    "            batchOut[j, :] = output_j\n",
    "            \n",
    "            #action = actions.iloc[actionAfter-1, :].values\n",
    "            #analogAction = action[:actionAnalogSz].astype(np.float32)\n",
    "            #if (len(np.where(np.isnan(analogAction))[0]) > 0):\n",
    "            #    print (\"BAD ACTION! \", actions.iloc[actionAfter-1, :])\n",
    "            #statesWithHistory = states.iloc[actionAfter - history:actionAfter, :].values.ravel()\n",
    "            #dataBatches.append({\n",
    "            #    'state': torch.from_numpy(statesWithHistory).float(),\n",
    "            #    'analogAction': torch.from_numpy(action[:actionAnalogSz].astype(np.float32)).float(),\n",
    "            #    'digitalAction': torch.from_numpy(action[actionAnalogSz:].astype(np.bool)).float()\n",
    "            #})\n",
    "            \n",
    "        dataBatches.append({\n",
    "            'input': torch.from_numpy(batchIn).float(),\n",
    "            'output': torch.from_numpy(batchOut).float(),\n",
    "        })\n",
    "            \n",
    "            \n",
    "    return dataBatches, stateSz, actionSz\n",
    "    \n",
    "\n",
    "def runTraining():    \n",
    "    dataBatches, stateSz, actionSz = dataToBatches(P2S, P2A, BATCH_SZ)\n",
    "    model = MyModel(stateSz, actionSz)\n",
    "    train_future_predict(dataBatches, model)\n",
    "    model.save(\"fp_BoostOnly1v1\")\n",
    "    return model\n",
    "\n",
    "model = runTraining()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
